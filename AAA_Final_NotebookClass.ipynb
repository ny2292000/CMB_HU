{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1feb0543-99aa-43eb-ac22-079f47ac959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from difflib import SequenceMatcher\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "class NotebookProcessor:\n",
    "    def __init__(self, notebook_path):\n",
    "        self.notebook_path = notebook_path\n",
    "        self.notebook_data = None\n",
    "        self.urls = []\n",
    "        self.downloaded_files = []  # Track downloaded files\n",
    "        self.load_notebook()\n",
    "\n",
    "    def load_notebook(self):\n",
    "        \"\"\"Loads the notebook content into memory.\"\"\"\n",
    "        with open(self.notebook_path, 'r') as file:\n",
    "            self.notebook_data = json.load(file)\n",
    "\n",
    "    def save_notebook(self, save_path=None):\n",
    "        \"\"\"Saves the notebook content from memory back to file.\"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = self.notebook_path\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(self.notebook_data, file, indent=2)\n",
    "        \n",
    "    def clean_downloaded_files(self):\n",
    "        # Delete downloaded files\n",
    "        for file_path in self.downloaded_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Successfully deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "    \n",
    "    def clean(self, directories_to_clean=[]):\n",
    "        \"\"\"Cleans all outputs from code cells in the notebook, deletes downloaded files, and clears specified directories.\"\"\"\n",
    "        if self.notebook_data is None:\n",
    "            print(\"Notebook data is not loaded.\")\n",
    "            return self\n",
    "\n",
    "        # Clean code cell outputs\n",
    "        for cell in self.notebook_data['cells']:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                cell['outputs'] = []\n",
    "                cell['execution_count'] = None\n",
    "        \n",
    "        self.clean_downloaded_files()\n",
    "        \n",
    "        # Clear specified directories\n",
    "        for directory in directories_to_clean:\n",
    "            if os.path.exists(directory):\n",
    "                for filename in os.listdir(directory):\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                        print(f\"Removed: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        \n",
    "        self.save_notebook()\n",
    "\n",
    "    def download_files(self, urls, directory):\n",
    "        self.urls = urls\n",
    "        \"\"\"Downloads files from the given URLs into the specified directory.\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)  # Ensure the directory exists\n",
    "        downloaded_files = []\n",
    "        for url in self.urls:\n",
    "            try:\n",
    "                local_filename = url.split('/')[-1]  # Extract the file name from the URL\n",
    "                local_filepath = os.path.join(directory, local_filename)  # Full local path\n",
    "                downloaded_files.append(local_filepath)\n",
    "                with requests.get(url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(local_filepath, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                self.downloaded_files.append(local_filepath)  # Track downloaded file\n",
    "                print(f\"Downloaded and saved: {local_filepath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "        self.download_files= downloaded_files\n",
    "\n",
    "    def export_to_py(self, output_path):\n",
    "        \"\"\"Exports notebook code cells to a Python (.py) file.\"\"\"\n",
    "        with open(output_path, 'w') as py_file:\n",
    "            for cell in self.notebook_data['cells']:\n",
    "                if cell['cell_type'] == 'code':\n",
    "                    py_file.write(''.join(cell['source']) + '\\n\\n')\n",
    "        print(f\"Python file created at: {output_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def find_notebooks(directory):\n",
    "        \"\"\"Recursively finds all notebooks in a directory.\"\"\"\n",
    "        notebooks = []\n",
    "        for root, _, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "                notebooks.append(os.path.join(root, filename))\n",
    "        return notebooks\n",
    "\n",
    "    @staticmethod\n",
    "    def cell_similarity(cell_a, cell_b):\n",
    "        \"\"\"Calculates the similarity of two cells using a simple ratio.\"\"\"\n",
    "        return SequenceMatcher(None, cell_a, cell_b).ratio()\n",
    "\n",
    "    @classmethod\n",
    "    def compare_notebooks(cls, notebook_path_a, notebook_path_b):\n",
    "        \"\"\"Compares two notebooks and returns the similarity metrics.\"\"\"\n",
    "        with open(notebook_path_a, 'r') as file_a, open(notebook_path_b, 'r') as file_b:\n",
    "            notebook_a = json.load(file_a)\n",
    "            notebook_b = json.load(file_b)\n",
    "        \n",
    "        identical, similar, distinct = 0, 0, 0\n",
    "        for cell_a in notebook_a['cells']:\n",
    "            if cell_a['cell_type'] == 'code':\n",
    "                for cell_b in notebook_b['cells']:\n",
    "                    if cell_b['cell_type'] == 'code':\n",
    "                        similarity = cls.cell_similarity(''.join(cell_a['source']), ''.join(cell_b['source']))\n",
    "                        if similarity == 1:\n",
    "                            identical += 1\n",
    "                        elif similarity >= 0.6:  # Threshold for \"similarity\"\n",
    "                            similar += 1\n",
    "                        else:\n",
    "                            distinct += 1\n",
    "        return identical, similar, distinct\n",
    "\n",
    "    # @staticmethod\n",
    "    # def find_notebooks(directory):\n",
    "    #     \"\"\"Recursively finds all notebook files in the specified directory, excluding '.ipynb_checkpoints'.\"\"\"\n",
    "    #     notebooks = []\n",
    "    #     for root, dirs, files in os.walk(directory):\n",
    "    #         # Skip any directories named '.ipynb_checkpoints'\n",
    "    #         dirs[:] = [d for d in dirs if d != '.ipynb_checkpoints']\n",
    "    #         for file in files:\n",
    "    #             if file.endswith(\".ipynb\"):\n",
    "    #                 notebooks.append(os.path.join(root, file))\n",
    "    #     return notebooks\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_notebooks(directory):\n",
    "        \"\"\"Finds all notebook files in the specified directory, excluding '.ipynb_checkpoints', without recursing into subdirectories.\"\"\"\n",
    "        notebooks = []\n",
    "        # Ensure not to consider '.ipynb_checkpoints' by filtering directories\n",
    "        for file in os.listdir(directory):\n",
    "            filepath = os.path.join(directory, file)\n",
    "            if file.endswith(\".ipynb\") and '.ipynb_checkpoints' not in filepath:\n",
    "                notebooks.append(filepath)\n",
    "        return notebooks\n",
    "\n",
    "    @classmethod\n",
    "    def compare_notebooks(cls, notebook_path_a, notebook_path_b):\n",
    "        \"\"\"Compares two notebooks based on their code cells content.\"\"\"\n",
    "        # Load notebooks\n",
    "        with open(notebook_path_a, 'r', encoding='utf-8') as file_a:\n",
    "            notebook_a = json.load(file_a)\n",
    "        with open(notebook_path_b, 'r', encoding='utf-8') as file_b:\n",
    "            notebook_b = json.load(file_b)\n",
    "        \n",
    "        # Initialize counts\n",
    "        identical, similar, distinct = 0, 0, 0\n",
    "        \n",
    "        # Extract and compare code cells\n",
    "        code_cells_a = [cell['source'] for cell in notebook_a['cells'] if cell['cell_type'] == 'code']\n",
    "        code_cells_b = [cell['source'] for cell in notebook_b['cells'] if cell['cell_type'] == 'code']\n",
    "        \n",
    "        # Prepare for comparison\n",
    "        for cell_a in code_cells_a:\n",
    "            cell_a_content = ''.join(cell_a).strip()\n",
    "            if not cell_a_content:  # Skip empty cells\n",
    "                continue\n",
    "            best_match = 0  # Track the best match for this cell\n",
    "            for cell_b in code_cells_b:\n",
    "                cell_b_content = ''.join(cell_b).strip()\n",
    "                if not cell_b_content:  # Skip empty cells\n",
    "                    continue\n",
    "                similarity = SequenceMatcher(None, cell_a_content, cell_b_content).ratio()\n",
    "                if similarity > best_match:\n",
    "                    best_match = similarity\n",
    "            if best_match == 1:\n",
    "                identical += 1\n",
    "            elif best_match > 0:\n",
    "                similar += 1\n",
    "            else:\n",
    "                distinct += 1\n",
    "        \n",
    "        if notebook_path_a == notebook_path_b:\n",
    "            # Adjust counts for self-comparison to consider non-empty cells only\n",
    "            non_empty_cells = sum(1 for cell in code_cells_a if ''.join(cell).strip())\n",
    "            identical = non_empty_cells\n",
    "            similar = 0\n",
    "            distinct = 0\n",
    "        \n",
    "        return identical, similar, distinct\n",
    "\n",
    "    @classmethod\n",
    "    def generate_comparison_matrix(cls, directory):\n",
    "        notebooks = cls.find_notebooks(directory)\n",
    "        comparison_results = []\n",
    "\n",
    "        for i, notebook_a in enumerate(notebooks):\n",
    "            for notebook_b in notebooks[i+1:]:\n",
    "                identical, similar, distinct = cls.compare_notebooks(notebook_a, notebook_b)\n",
    "                comparison_results.append((notebook_a, notebook_b, identical, similar, distinct))\n",
    "        \n",
    "        return comparison_results\n",
    "\n",
    "    @classmethod\n",
    "    def save_comparisons_to_txt(cls, comparison_results, output_path):\n",
    "        \"\"\"Saves the comparison results to a text file.\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            for result in comparison_results:\n",
    "                f.write(f\"{result[0]}, {result[1]}, Identical: {result[2]}, Similar: {result[3]}, Distinct: {result[4]}\\n\")\n",
    "\n",
    "    @classmethod\n",
    "    def save_comparisons_to_excel(cls, comparison_results, output_path):\n",
    "        \"\"\"Saves the comparison results to an Excel file.\"\"\"\n",
    "        # Create a DataFrame from the comparison results\n",
    "        df = pd.DataFrame(comparison_results, columns=['Notebook 1', 'Notebook 2', 'Identical Cells', 'Similar Cells', 'Distinct Cells'])\n",
    "        \n",
    "        # Save the DataFrame to an Excel file\n",
    "        df.to_excel(output_path, index=False)\n",
    "        print(f\"Comparison results saved to {output_path}\")\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_graph(cls, comparison_results, similarity_threshold):\n",
    "        \"\"\"Generates and displays a graph based on the comparison results and a similarity threshold.\"\"\"\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add edges for notebook pairs that meet the similarity threshold\n",
    "        for nb1, nb2, identical, similar, distinct in comparison_results:\n",
    "            if identical + similar >= similarity_threshold:\n",
    "                G.add_node(nb1, label=os.path.basename(nb1))\n",
    "                G.add_node(nb2, label=os.path.basename(nb2))\n",
    "                G.add_edge(nb1, nb2, weight=identical + similar)\n",
    "\n",
    "        # Draw the graph\n",
    "        pos = nx.spring_layout(G, k=0.5, iterations=20)\n",
    "        nx.draw(G, pos, with_labels=True, node_size=2000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\")\n",
    "        edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c07f561-d939-49a4-af2f-183fada7cde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: ./DataSupernovaLBLgov/COM_PowerSpect_CMB-base-plikHM-TTTEEE-lowl-lowE-lensing-minimum-theory_R3.01.txt\n",
      "Downloaded and saved: ./DataSupernovaLBLgov/COM_CMB_IQU-smica_1024_R2.02_full.fits\n"
     ]
    }
   ],
   "source": [
    "# if __name__==\"__main__\":\n",
    "dirs = [\"./img\", \"./img1\", \"./universeorthoviewdual\", \"./universeorthoview_48_48\", \"./PG_data\", \"./universeorthoview\"]\n",
    "# Directory to which we want to save the files\n",
    "directory = './DataSupernovaLBLgov'\n",
    "# URLs of the files to download\n",
    "urls = [\n",
    "    'https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/cosmoparams/COM_PowerSpect_CMB-base-plikHM-TTTEEE-lowl-lowE-lensing-minimum-theory_R3.01.txt',\n",
    "    'https://irsa.ipac.caltech.edu/data/Planck/release_2/all-sky-maps/maps/component-maps/cmb/COM_CMB_IQU-smica_1024_R2.02_full.fits'\n",
    "]\n",
    "# notebook1 = NotebookProcessor(\"./AAAA1_AWS_UniverseMap-GoldenCopy.ipynb\")\n",
    "# notebook1.download_files(urls=urls, directory=directory)\n",
    "# notebook1.clean_downloaded_files()\n",
    "# notebook1.clean(directories_to_clean=dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ba7016-9c97-4cb1-b982-74ae2baf9a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python file created at: ./AAA_Final_CMB_Modeling_UniverseMap.py\n"
     ]
    }
   ],
   "source": [
    "# notebook1 = NotebookProcessor(\"./AAA_Final_CMB_Modeling_UniverseMap.ipynb\")\n",
    "# notebook1.export_to_py(\"./AAA_Final_CMB_Modeling_UniverseMap.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e72d7a6-8bcc-492c-887c-f5a0eb880534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "directory = './'\n",
    "# Assuming the generate_comparison_matrix and other necessary methods are defined within the class\n",
    "comparison_results = NotebookProcessor.generate_comparison_matrix(directory)\n",
    "# NotebookProcessor.save_comparisons_to_txt(comparison_results, output_path)\n",
    "# NotebookProcessor.generate_graph(comparison_results, similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c13e21b-83b0-4071-b579-791845d94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_txt = './comparison_results.txt'\n",
    "NotebookProcessor.save_comparisons_to_txt(comparison_results, output_path_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5beeeb4-0e07-4715-a0ae-b91fa61781e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results saved to ./comparison_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "output_path_xls = './comparison_results.xlsx'\n",
    "NotebookProcessor.save_comparisons_to_excel(comparison_results, output_path_xls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50203cb-2ae8-40f4-a462-c31cc1e95ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 115  # Define a threshold for similarity\n",
    "NotebookProcessor.generate_graph(comparison_results, similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baeabff-19a0-4d2b-807f-761ad7e84546",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = \"./AAAA1_AWS_UniverseMap-GoldenCopy.ipynb\"\n",
    "identical, similar, distinct = NotebookProcessor.compare_notebooks(notebook_path, notebook_path)\n",
    "\n",
    "print(f\"Identical: {identical}, Similar: {similar}, Distinct: {distinct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa6178-c07b-45f6-b8be-f40452ed0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook1.export_to_py(\"./CMB_HU_latest_to_git.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3849dd4-4cc0-4c23-8faf-73d192846766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming compare_notebooks returns similarity score and we have a list of tuples: (notebook1, notebook2, similarity_score)\n",
    "# Example: [('nb1.ipynb', 'nb2.ipynb', 5), ...]\n",
    "directory = './'\n",
    "comparison_results = NotebookProcessor.generate_comparison_matrix(directory)\n",
    "# print(comparison_results)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between notebooks with weights based on similarity\n",
    "for nb1, nb2, similarity,s2,s3 in comparison_results:\n",
    "    if similarity > 0:  # Assuming we only care about notebooks with some similarity\n",
    "        G.add_edge(nb1, nb2, weight=similarity)\n",
    "\n",
    "# Draw the network\n",
    "pos = nx.spring_layout(G)  # Positions for all nodes\n",
    "\n",
    "# Nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "\n",
    "# Edges\n",
    "weights = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edges(G, pos, width=list(weights.values()))\n",
    "\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c032a68-aa9a-4e47-b3a8-8a652df33b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c035f84-a70d-41c0-8242-37ae2e1e114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# !swapoff -a\n",
      "---\n",
      "# ! python -mpip install git+https://github.com/qutip/qutip-cupy.git\n",
      "---\n",
      "# ! python -mpip install git+https://github.com/qutip/qutip.git@dev.major\n",
      "---\n",
      "from lib4 import *\n",
      "import numpy as np\n",
      "from scipy.optimize import curve_fit\n",
      "mypath = \"./PG_data\"\n",
      "x0 = [4.93231069, 4.97130803, 0.85524497] # interrupted optimized value 03/27/2024\n",
      "---\n",
      "todo =[\n",
      "#          Color.FINDNEIGHBORHOOD,\n",
      "         Color.EVALUATE_DF_AT_POSITION,\n",
      "#          Color.MINIMIZEPOSITION,\n",
      "         Color.CREATE_HIGH_RESOL_BACKGROUNDPLOT,\n",
      "         Color.OPTIMIZE_SPECTRUM,\n",
      "         Color.OPTIMIZE_SMICA_BACKGROUND,\n",
      "         Color.CREATE_GAUSSIAN_BACKGROUND,\n",
      "         Color.CREATE_HISTOGRAM,\n",
      "#          Color.CHARACTERIZE_SPECTRUM \n",
      "#          Color.CREATEMAPOFUNIVERSE,\n",
      "#          Color.FINDBESTFORKRANGE,\n",
      "#          Color.CREATE_VARIABLE_R_GIF,\n",
      "#          Color.WORK_86_128,\n",
      "]\n",
      "\n",
      "\n",
      "---\n",
      "if Color.FINDNEIGHBORHOOD in todo:\n",
      "    # these three indices are related to position within the hyperspherical hypersurface.  Don't confuse them with the\n",
      "    # quantum numbers k,l,m\n",
      "    x0 = [0.0, 0.0, 0.0]\n",
      "    (lambda_k, lambda_l, lambda_m) = x0\n",
      "    # Create karray\n",
      "    nside3D = 48\n",
      "    bandwidth = 48\n",
      "    karray = list(np.arange(2, 10))\n",
      "    print(len(karray))\n",
      "    kmax = max(karray)\n",
      "    #################################\n",
      "    myHyper = HYPER(nside3D, sigma_smica, planck_IQU_SMICA, karray,mypath,\n",
      "                        lambda_k, lambda_l, lambda_m, loadpriorG=False,\n",
      "                        savePG=False, bandwidth=bandwidth)\n",
      "\n",
      "    #################################################################\n",
      "    # these three indices are related to position within the hyperspherical hypersurface.  Don't confuse them with the\n",
      "    # quantum numbers k,l,m\n",
      "    errarray = []\n",
      "    olderr = 1110.0\n",
      "    results = 0.0\n",
      "    x00 = []\n",
      "    nside3D = 48\n",
      "    bandwidth = 48\n",
      "    n = 7\n",
      "    myHyper.change_SMICA_resolution(nside3D, doit=False, bandwidth=bandwidth)\n",
      "    for lk in np.linspace(0, 2 * np.pi, n):\n",
      "        for ll in np.linspace(0, 2 * np.pi, n):\n",
      "            for lm in np.linspace(0, 2 * np.pi, n):\n",
      "                start_time = time()\n",
      "                try:\n",
      "                    myHyper.change_HSH_center(lk, ll, lm, karray, nside3D, loadpriorG=False, doit=True,\n",
      "                                              savePG=False)\n",
      "                    results, fcolors, err = myHyper.project4D3d_0(karray)\n",
      "                    myHyper.plotNewMap(fcolors,kmax, err, filename=None, title=None, plotme=False, save=True)\n",
      "                    if olderr > err:\n",
      "                        olderr = err\n",
      "                        err0 = np.round(err, 3)\n",
      "                        filename = \"./img1/Bestf_{}_{}_{}__{}_{}_{}.png\".format(myHyper.kmax, myHyper.nside3D,\n",
      "                                                                                err0,\n",
      "                                                                                chg2ang(lk),\n",
      "                                                                                chg2ang(ll),\n",
      "                                                                                chg2ang(lm))\n",
      "                        myHyper.plotNewMap(fcolors, err, filename=filename, title=None, plotme=True, save=True)\n",
      "                        x00.append((lk, ll, lm, err))\n",
      "                        np.save(\"./img1/x0_{}_{}.npy\".format(kmax, nside3D), x00[-1:], allow_pickle=True)\n",
      "                        print(lk, ll, lm, err)\n",
      "                except Exception as aa:\n",
      "                    print('Error with getting map for: {}_{}_{}'.format(lk, ll, lm))\n",
      "                stop_time = time()\n",
      "    myHyper.creategiff(kmax, nside3D, mypath=\"./img1\", prefix=\"aitoff_\", filename=\"CMB\")\n",
      "    sleep(10)\n",
      "    myHyper.creategiff(kmax, nside3D, mypath=\"./img1\", prefix=\"Bestf\", filename=\"BEST\")\n",
      "    sleep(10)\n",
      "    myHyper.cleanup(mypath=\"./img1\", prefix=\"aitoff_\")\n",
      "    # myHyper.cleanup(mypath=\"./img1\", prefix=\"Bestf\")\n",
      "    print(\"Best Position = \", x00[-1:])\n",
      "    bestposition = \"./img1/x0_{}_{}.npy\".format(kmax, nside3D)\n",
      "    np.save(\"./img1/x0_{}_{}.npy\".format(kmax, nside3D), x00[-1:], allow_pickle=True)\n",
      "    np.save(\"./img1/x00_{}_{}.npy\".format(kmax, nside3D), x00, allow_pickle=True)\n",
      "---\n",
      "if Color.MINIMIZEPOSITION in todo:\n",
      "    x0 = [4.93231069, 4.97130803, 0.85524497] # interrupted optimized value 03/27/2024\n",
      "    \n",
      "    nside3D = 64\n",
      "    bandwidth = 64\n",
      "    (lambda_k, lambda_l, lambda_m) = x0\n",
      "    # Create karray\n",
      "    karray = list(np.arange(2, 30))\n",
      "    print(len(karray))\n",
      "    kmax = max(karray)\n",
      "    #################################\n",
      "    myHyper = HYPER(nside3D, sigma_smica, planck_IQU_SMICA, karray,mypath,\n",
      "                        lambda_k, lambda_l, lambda_m, loadpriorG=False, savePG=False, bandwidth=bandwidth)\n",
      "    print(x0)\n",
      "    myHyper.change_SMICA_resolution(nside3D, doit=False, bandwidth=bandwidth)\n",
      "    x0 = minimize(myHyper.calcError, x0, args=(karray,nside3D), method='nelder-mead',\n",
      "                  options={'xatol': 1e-4, 'disp': True})\n",
      "    np.save(\"./img1/x0_{}_{}.npy\".format(kmax, nside3D), x0.x, allow_pickle=True)\n",
      "    x0 = x0.x\n",
      "    (lk, ll, lm) = [x+1 for x in x0]\n",
      "    print(\"minimized at {}\".format(x0))\n",
      "    filename = \"./img1/Best_{}_{}_{}__{}_{}_{}.png\".format(kmax, nside3D, bandwidth, chg2ang(lk), chg2ang(ll),\n",
      "                                                           chg2ang(lm))\n",
      "    title = \"Best_{}_{}_{}__{}_{}_{}\".format(kmax, nside3D, bandwidth, chg2ang(lk), chg2ang(ll), chg2ang(lm))\n",
      "    myHyper.change_HSH_center(lk, ll, lm, karray, nside3D, loadpriorG=False, doit=True, savePG=True)\n",
      "    results, newmap, err = myHyper.project4D3d_0(karray)\n",
      "    np.save(\"./img1/results_{}_{}_{}.npy\".format(kmax, nside3D, bandwidth), results, allow_pickle=True)\n",
      "    (lk, ll, lm) = [x+1 for x in x0]\n",
      "    filename = \"./img1/BestMaximized_{}_{}_{}__{}_{}_{}.png\".format(kmax, nside3D, bandwidth, chg2ang(lk), chg2ang(ll),\n",
      "                                                           chg2ang(lm))\n",
      "    title = \"BestMaximized_{}_{}_{}__{}_{}_{}\".format(kmax, nside3D, bandwidth, chg2ang(lk), chg2ang(ll), chg2ang(lm))\n",
      "    myHyper.plotNewMap(newmap, err, filename=filename, title=title, plotme=True, save=True)\n",
      "    print(\"maximized at \", x0)\n",
      "---\n",
      "if Color.EVALUATE_DF_AT_POSITION in todo:\n",
      "    myHyper = None\n",
      "    # Earth Position\n",
      "    x0 = [4.93231069, 4.97130803, 0.85524497] # interrupted optimized value 03/27/2024\n",
      "    y0 = [np.round((xx + 1) / np.pi * 180, 2) for xx in x0]\n",
      "    print(\"Earth Position\", y0)\n",
      "    (lk, ll, lm) = x0\n",
      "\n",
      "    print(\"evaluated at {}\".format(x0))\n",
      "    # Create karray\n",
      "    karray = list(np.arange(2, 48))\n",
      "\n",
      "    nside3D = 64\n",
      "    bandwidth = 64\n",
      "    print(len(karray))\n",
      "    kmax = max(karray)\n",
      "    #################################\n",
      "    myHyper = HYPER(nside3D, sigma_smica, planck_IQU_SMICA, karray ,mypath,\n",
      "                        lk, ll, lm, loadpriorG=False, savePG=False, bandwidth=bandwidth, longG=False)\n",
      "    #################################################################\n",
      "    results, newmap0, err = myHyper.project4D3d_0(karray)\n",
      "    #########################################################################\n",
      "    (lk, ll, lm) = [x+1 for x in x0]\n",
      "    filename = \"./img1/Best_{}_{}_{}__{}_{}_{}.png\".format(kmax, nside3D, bandwidth, chg2ang(lk), chg2ang(ll),\n",
      "                                                           chg2ang(lm))\n",
      "    title = \"Best_{}_{}_{}__{}_{}_{}\".format(kmax, nside3D, bandwidth, chg2ang(lk), chg2ang(ll), chg2ang(lm))\n",
      "    myHyper.plotNewMap(newmap0, err, filename=filename, title=title, plotme=True, save=True)\n",
      "\n",
      "---\n",
      "if Color.CREATE_HIGH_RESOL_BACKGROUNDPLOT in todo:\n",
      "    nside3D=1024 # 4096\n",
      "    newmap0_lg = myHyper.change_resolution(newmap0.squeeze(), nside3D=nside3D, plotme=True, title= \"BestFit\", bandwidth=64)\n",
      "    SMICA_lg = myHyper.change_resolution(myHyper.SMICA.squeeze(), nside3D=nside3D, plotme=True,title= \"SMICA\", bandwidth=nside3D)\n",
      "    # SMICA_lg = myHyper.SMICA.squeeze()\n",
      "    # diffmap_lg = SMICA_lg - newmap0_lg\n",
      "    # myHyper.plotNewMap(diffmap_lg, err, filename=None, title=\"Difference Map\", plotme=True, save=False, nosigma=False)\n",
      "    \n",
      "    x01 = np.array([0.00161687, 0.31435231])\n",
      "    x00 = minimize(newerr, x01, args=(SMICA_lg, newmap0_lg),\n",
      "                   method='nelder-mead', options={'xatol': 1e-8, 'disp': True})\n",
      "    err = x00.fun\n",
      "    xx0 = x00.x\n",
      "    newmap0_lg =xx0[1]*newmap0_lg  #+ xx0[0]\n",
      "    diffmap_lg = newmap0_lg- SMICA_lg\n",
      "    (mu, sigma) = norm.fit(diffmap_lg)\n",
      "    if sigma != 0.0:\n",
      "        diffmap_lg = diffmap_lg / sigma * myHyper.sigma_smica           \n",
      "    myHyper.plotNewMap(diffmap_lg, err, filename=None, title=\"Difference Map\", plotme=True, save=False, nosigma=False)\n",
      "    np.save(\"./img1/diffmap.npy\", diffmap_lg )\n",
      "---\n",
      "if Color.CREATE_HISTOGRAM in todo:\n",
      "    # Create Histogram\n",
      "#     x01 = np.array([0.00161687, 0.31435231])\n",
      "#     x00 = minimize(newerr, x01, args=(myHyper.SMICA_LR.squeeze(), newmap0.squeeze()),\n",
      "#                    method='nelder-mead', options={'xatol': 1e-8, 'disp': True})\n",
      "#     err = x00.fun\n",
      "#     xx0 = x00.x\n",
      "#     newmap0_lg\n",
      "#     newmap, diffmap = myHyper.optimizeNewMap(newmap0.squeeze(), myHyper.SMICA_LR.squeeze(),\n",
      "#                                                                xx0=xx0,\n",
      "#                                                                nside3D=nside3D, bandwidth=bandwidth, nosigma=True)\n",
      "    myHyper.plotHistogram(newmap0_lg, nside3D, kmax, plotme=True)\n",
      "    #################################################################\n",
      "    #################################################################\n",
      "    #################################################################\n",
      "    #########################################################################     \n",
      "        \n",
      "    filename = \"./img1/SingleBestOptimizedColor_{}_{}_{}__{}_{}_{}.png\".format(kmax, nside3D, bandwidth, chg2ang(lk),\n",
      "                                                                 chg2ang(ll), chg2ang(lm))\n",
      "    title = \"BestOptimizedColor_{}_{}_{}_{}__{}_{}_{}\".format(kmax, nside3D, bandwidth, err, chg2ang(lk),\n",
      "                                                chg2ang(ll), chg2ang(lm))\n",
      "\n",
      "    myHyper.plotNewMap(newmap0_lg, err, filename=filename, title=title, plotme=True, save=True, nosigma=False)\n",
      "    myHyper.plotNewMap(myHyper.SMICA_LR, err, filename=None, title=\"SMICA\", plotme=True, save=False, nosigma=False)\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "def print_first_cells(notebook_path, num_cells=3):\n",
    "    \"\"\"Print the source of the first few code cells in the notebook.\"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "        code_cells = [cell for cell in nb.cells if cell.cell_type == 'code']\n",
    "        for cell in code_cells[:num_cells]:\n",
    "            print(''.join(cell.source))\n",
    "            print('---')  # Separator\n",
    "\n",
    "# Example usage:\n",
    "print_first_cells('./AAAA1_AWS_UniverseMap-GoldenCopy.ipynb',10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb57fb0-fc3a-4bfc-8e9c-8188d19a5dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook A cell content:\n",
      "basis(5,3)\n",
      "\n",
      "Notebook B cell content:\n",
      "if Color.OPTIMIZE_SMICA_BACKGROUND in todo:\n",
      "    beam_arc_min=10\n",
      "    white_noise = np.ma.asarray(np.random.normal(0, sigma_smica, 12*nside3D** 2))\n",
      "    cl_WHITE_NOISE, dl_WHITE_NOISE, ell = get_dl(white_noise, nside=nside3D, beam_arc_min=1)\n",
      "    cl_SMICA, dl_SMICA, ell = get_dl(diffmap_lg.squeeze(), nside=nside3D, beam_arc_min=beam_arc_min)\n",
      "    def yy_func(x, ell):\n",
      "        return x[0] + x[1] * np.exp(ell * x[2] + ell**2 *x[3])\n",
      "    \n",
      "    def olderr(x, dl_SMICA, ell):\n",
      "        err = dl_SMICA - yy_func(x,ell)\n",
      "        err = np.sum(err * err)\n",
      "        print(err, x)\n",
      "        return err\n",
      "\n",
      "\n",
      "    dl_SMICA1 = xx0[1]*dl_SMICA+xx0[0]\n",
      "    x01 = np.array([ 0.00618878,  0.00087167,  0.00037092, -0.002705  , -0.00031672])\n",
      "    aa = 2000\n",
      "    bb= 2800\n",
      "    x = ell[aa:bb].squeeze()\n",
      "    y = dl_SMICA1[aa:bb].squeeze()\n",
      "    x00 = minimize(olderr, x01, args=(y, x),\n",
      "                   method='nelder-mead', options={'xatol': 1e-6, 'disp': True})\n",
      "    err = x00.fun\n",
      "    xx0 = x00.x\n",
      "    yy = yy_func(xx0, ell) \n",
      "    dl_SMICA_Clean = dl_SMICA1 - yy\n",
      "    # a0 = pars[0]  # Amplitude\n",
      "    # a1 = pars[1]  # center\n",
      "    # a2 = pars[2]  # std\n",
      "    # parguess = [326.07961712,   240.5495938,    130.86283279,\n",
      "    #             185.2806624, 511.71738065,   106.82439558,\n",
      "    #             654.09021179,   778.65904957, 195.83549305,\n",
      "    #             50.093336,    1114.17415044,   102.05325759,\n",
      "    #             105.70709484,  1272.23029976,   184.05430212,\n",
      "    #             -1185.72051889, 666.00139292,   367.6208938 ]\n",
      "\n",
      "    mygauss = fitClass()\n",
      "    mygauss.n=5\n",
      "    parguess = np.array([1.13141056e+02, 2.54610864e+02, 8.41771175e+01, 4.92402013e-03,\n",
      "                         9.80458369e+02, 5.95410382e+02, 8.96299871e+01, 7.51841770e-03,\n",
      "                         2.60752925e+01, 8.22194747e+02, 9.18280463e+01, 7.75142569e-04,\n",
      "                         4.60311565e+01, 1.14621194e+03, 8.47451004e+01, 2.12699957e-03,\n",
      "                         4.00537143e-02, 1.41213361e+03, 6.14342868e+01, -2.42490682e-03])\n",
      "    plt.plot(ell, mygauss.six_peaks(ell, *parguess), 'r-')\n",
      "    plt.show()\n",
      "    np.save(\"./PG_data/ell.npy\", ell)\n",
      "    np.save(\"./PG_data/dl_SMICA_Clean.npy\", dl_SMICA_Clean)\n",
      "    popt, _ = curve_fit(mygauss.six_peaks, ell[0:2000], dl_SMICA_Clean[0:2000], parguess, maxfev = 420000)\n",
      "    print(popt)\n",
      "    parguess = np.array(popt).reshape([5, 4])\n",
      "    gamma= popt[-1:][0]\n",
      "    # parguess = parguess[parguess[:, 1].argsort()]\n",
      "    centers = np.array([parguess[x, 1] for x in np.arange(5)])\n",
      "    freqs = centers[1::] - centers[0:-1:]\n",
      "    amplt = np.array([parguess[x, 0] for x in np.arange(5)])\n",
      "#     fitting1 = np.polyfit(centers, np.log(amplt), 1)\n",
      "#     yy = np.exp(fitting1[1]) * np.exp(fitting1[0] * ell)\n",
      "#     # First peak is at pi/2=l*Delta => Delta=pi/2/l\n",
      "#     delta = np.pi / 2 / centers[0]\n",
      "#     gamma1 = fitting1[0] / delta\n",
      "\n",
      "# #     ################################################################\n",
      "# #     ################################################################\n",
      "#     fig, axis1 = plt.subplots()\n",
      "#     axis1.plot(ell, dl_SMICA_Clean)\n",
      "#     for i in np.arange(5):\n",
      "#         axis1.plot(ell, mygauss.fitfun(ell, *parguess[i, :]), 'r-')\n",
      "#     axis1.plot(ell, mygauss.six_peaks(ell, *popt), 'r-')\n",
      "#     axis1.set_xlim([0, 2000])\n",
      "#     axis1.set_ylim([0, None])\n",
      "#     axis1.set_xlabel('Spherical Harmonic L')\n",
      "#     axis1.set_ylabel('Intensity (arb. units)')\n",
      "#     axis1.set_title(\"Dissipation on High-Fequency CMB Power Spectrum \\n delta ={}  Gamma={}\".format('%.4E', '%.4E') %\n",
      "#               (delta, gamma1))\n",
      "#     plt.legend(['Power Spectrum', 'Fitted Data'])\n",
      "#     plt.savefig('./img1/HighFreqFittedPowerSpectrum.png')\n",
      "#     plt.show()\n",
      "# #     ################################################################\n",
      "# #     ################################################################\n",
      "#     fig, axis1 = plt.subplots()\n",
      "#     fitting1 = np.polyfit(centers, np.log(amplt), 1)\n",
      "#     yy = np.exp(fitting1[1]) * np.exp(fitting1[0] * ell)\n",
      "#     # First peak is at pi/2=l*Delta => Delta=pi/2/l\n",
      "#     plt.scatter(centers, amplt)\n",
      "#     plt.plot(ell, yy)\n",
      "#     plt.xlim([0, 2000])\n",
      "#     # plt.ylim([-200, 2000])\n",
      "#     plt.xlabel('Spherical Harmonic L')\n",
      "#     plt.ylabel('Intensity (arb. units)')\n",
      "#     plt.title(\"Dissipation on High-Fequency CMB Power Spectrum \\n delta ={}  Gamma={}\".format('%.4E', '%.4E') % (\n",
      "#         delta, gamma1))\n",
      "#     plt.legend(['Power Spectrum', 'Fitted Data'])\n",
      "#     # plt.savefig('./img1/HighFreqDissipationFittedPowerSpectrum.png')\n",
      "#     plt.show()\n",
      "#     freq = [centers[i + 1] - centers[i] for i in np.arange(5)]\n",
      "#     freqdiff = [x / freq[0] for x in freq]\n",
      "#     print(freq, freqdiff)\n",
      "#     print(amplt)\n",
      "#     ################################################################\n",
      "#     ################################################################\n",
      "#     fitting2 = np.polyfit(centers[0:-1:] * delta, freq, 2)\n",
      "#     plt.scatter(centers, amplt)\n",
      "#     plt.plot(ell, yy)\n",
      "#     plt.xlim([0, 2000])\n",
      "#     plt.ylim([0, None])\n",
      "#     plt.xlabel('Spherical Harmonic L')\n",
      "#     plt.ylabel('Intensity (arb. units)')\n",
      "#     plt.title(\"Dissipation on High-Fequency CMB Power Spectrum \\n delta ={}  Gamma={}\".format('%.4E', '%.4E') % (\n",
      "#         delta, gamma))\n",
      "#     plt.legend(['Power Spectrum', 'Fitted Data'])\n",
      "#     plt.savefig('./img1/FreqFitteditted.png')\n",
      "#     plt.show()\n",
      "#     ################################################################\n",
      "#     ################################################################\n",
      "#     #    A = pars[0]  # sin amplitude\n",
      "#     # delta = pars[1]  # delta\n",
      "#     # gamma = pars[2]  #  gamma exponential damping term\n",
      "#     # h0 = pars[3]\n",
      "#     parguess = [1.75703174e+00,\n",
      "#                 -3.18905547e-01, 1.08942482e-02, -1.51470042e-05, 6.98842799e-09,\n",
      "#                 -7.17548356e-01, 5.52935906e-01, 2.00000000e+01]\n",
      "#     popt, _ = curve_fit(sindelta, ell[0:2000], dll_SMICA_Clean[0:2000], parguess)\n",
      "#     print(popt)\n",
      "#     parguess = np.array(popt)\n",
      "#     plt.figure()\n",
      "#     plt.plot(ell, dll_SMICA_Clean)\n",
      "#     plt.plot(ell, sindelta(ell, *popt), 'r-')\n",
      "#     plt.xlim([0, 2000])\n",
      "#     plt.ylim([0, None])\n",
      "#     plt.xlabel('Spherical Harmonic L')\n",
      "#     plt.ylabel('Intensity (arb. units)')\n",
      "#     plt.title(\"Modeling High-Fequency CMB Power Spectrum\")\n",
      "#     plt.legend(['Power Spectrum', 'Fitted Data'])\n",
      "#     plt.savefig('./img1/HighFreqFittedPowerSpectrum.png')\n",
      "#     plt.show()\n",
      "#     ################################################################\n",
      "#     ################################################################\n",
      "#     aaaa = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_specific_cells(notebook_path_a, notebook_path_b, cell_index):\n",
    "    \"\"\"Compare the source of a specific cell index between two notebooks.\"\"\"\n",
    "    with open(notebook_path_a, 'r', encoding='utf-8') as f:\n",
    "        nb_a = nbformat.read(f, as_version=4)\n",
    "    with open(notebook_path_b, 'r', encoding='utf-8') as f:\n",
    "        nb_b = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    cell_a = nb_a.cells[cell_index].source if cell_index < len(nb_a.cells) else \"Cell index out of range\"\n",
    "    cell_b = nb_b.cells[cell_index].source if cell_index < len(nb_b.cells) else \"Cell index out of range\"\n",
    "\n",
    "    # Print both cells for manual comparison\n",
    "    print(\"Notebook A cell content:\")\n",
    "    print(cell_a)\n",
    "    print(\"\\nNotebook B cell content:\")\n",
    "    print(cell_b)\n",
    "\n",
    "# Example usage:\n",
    "compare_specific_cells('./AAAA_qutip_A.ipynb', './AAAA1_AWS_UniverseMap-GoldenCopy.ipynb', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0eca7-98d1-4835-aac8-886d1986a4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Cosmos3020]",
   "language": "python",
   "name": "conda-env-Cosmos3020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
